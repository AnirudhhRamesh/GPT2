{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GPT (Transformer)\n",
    "\n",
    "Here we are going to build up the 'GPT' model which is based on a Transformer architecture from: https://arxiv.org/pdf/1706.03762\n",
    "\n",
    "Compared to Bengio/WaveNet, the Transformer model (in wishy-washy terms):\n",
    "- Introduces an 'attention' layer, which allows the existing context spend compute/time to 'communicate' with each other\n",
    "- Introduces a feed-forward layer, which allows the existing context spend compute/time to properly process all the 'communications' they received to find connections to other words.\n",
    "- Add in a normalization layer (LayerNorm rather than BatchNorm, i.e. we normalize across the feature dims rather than the batch dims. More stable for variable length contexts)\n",
    "\n",
    "We then repeat this block multiple times to fit our model better to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset in chars: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear\n"
     ]
    }
   ],
   "source": [
    "# First up, let's use a new dataset. We'll be using the shakespeare dataset from karpathy's makemore gpt episode.\n",
    "with open(\"input.txt\", 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(f\"Length of dataset in chars: {len(data)}\")\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocab size: 65\n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n",
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# Let's encode the dataset\n",
    "block_size = 256\n",
    "\n",
    "vocab = sorted(list(set(data)))\n",
    "vocab_size = len(vocab)\n",
    "print(''.join(vocab))\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "stoi = {c:i for i, c in enumerate(vocab)}\n",
    "itos = {i:c for i, c in enumerate(vocab)}\n",
    "\n",
    "print(stoi)\n",
    "print(itos)\n",
    "\n",
    "# For fun & extra practice, I reimplemented this using lambda like Karpathy does\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda x: ''.join([itos[ix] for ix in x])\n",
    "\n",
    "test = \"hi there\"\n",
    "print(encode(test))\n",
    "print(decode(encode(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "# Let's prepare the dataset & split into train/valid/test sets\n",
    "\n",
    "#1. Encode the entire dataset as numbers, store as a torch.tensor\n",
    "dataset = torch.tensor(encode(data))\n",
    "print(dataset[:4])\n",
    "\n",
    "# 2. Split the dataset into training and validation (also test, but karpathy doesn't have a test set so I'm sticking with train/val to compare)\n",
    "train_split = int(0.9*dataset.shape[0])\n",
    "\n",
    "train_data = dataset[:train_split]\n",
    "val_data = dataset[train_split:]\n",
    "\n",
    "#2. Generate X,Y pairs\n",
    "# a. We want to generate pairs of context length. Thus, on a single context block_size=8, our model makes 8 predictions.\n",
    "# This is more efficient (data loaded once to GPU, processed 8 times) and allows it to infer using variable length input\n",
    "\n",
    "# b. We want to use batch to batch together independent contexts. This will give us better utilization of the GPU (highly parallelizable).\n",
    "\n",
    "torch.manual_seed(1337) #To compare with Karpathy's generation\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #pick any int up to the last element - block_size\n",
    "    \n",
    "    # We generate a batch with batch_size independent/random rows, each of length block_size\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(xb) #B, T\n",
    "print(yb) #B, T\n",
    "\n",
    "# Now let's generate the batches for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "# Now, we'll be using PyTorch modules (rather than creating from scratch)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Predict the next token given the input token\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #embedding output size = vocab size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) #B, T, C\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # PyTorch works with B, C, T (instead of B, T, C) thus we need to change the view\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        for _ in range(max_new_tokens): #We iterate as many times as tokens we want (unlike before, we don't stop on a '.' token)\n",
    "            logits, loss = self(idx)\n",
    "            #With Bigram model, we only care about the last character\n",
    "            logits = logits[:, -1, :] #Batch, Time, Channel/Embedding. With bigram, we only select the last context\n",
    "            probs = torch.softmax(logits, dim=1) #Note: dim=1 because we want a softmax on the embeddings/distribution not the batch/time\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1, replacement=True)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out)\n",
    "print(loss)\n",
    "print(out.shape) #4, 8, 65 (B=4, T=8, C=65) where B is batch, T is 'time' and C=embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oU,pxIBFAYuxKXe.jeh\n",
      "sa!3MGFrSjuM:wX!?BTMl!.?,M:bQzPHpYfN!Cbo'MmtDxBkDD3SBjyFdmY'DOqkWeRjlxyJB-bVbfd&\n"
     ]
    }
   ],
   "source": [
    "idx = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-4) #Can also use higher lr for smaller networks (e.g. 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6773767471313477\n"
     ]
    }
   ],
   "source": [
    "# Let's start training the model\n",
    "batch_size=32\n",
    "for _ in range(10000):\n",
    "\n",
    "    # Load a random sample\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "woaterirsth!atsigmpre!Yke n my;\n",
      "Angdsbou se ngro'yYCO:\n",
      "h RIs stine tchu nes oiVDWh My q ANCK:pe cth h ske thity to'ravir igUCXwer dul.\n",
      "Rose'ror.W:MftiswIrower.\n",
      "W?Y peyergise w, han kW:3: lvesk\n",
      "\n",
      "MAas dsereYoonond, thoullqNhagmet maistf hyfre w;RThe!UK:woom zes seakzzkHe p,\n",
      "RIOR:\n",
      "ofqm dmpptoXJOLLLLIN:Cybannclve s;\n",
      "\n",
      "\n",
      "OULItyatre sw, makilof WBandveZELCAhetrengratswh,\n",
      "s cueathureat gheiest nthabuis nd \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx, 400)[0].tolist()))\n",
    "\n",
    "#Output obviously very bad (for bigram model), but we'll improve it using transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Messing around with self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C) #batch time channels\n",
    "x.shape\n",
    "\n",
    "# We want the tokens to talk with each other. We only want the 5th token to speak with 1-4th (previous context -> current)\n",
    "# Easiest way -> just average the past tokens + current (\"current context of me with the past history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "xbow = torch.zeros((B, T, C)) #bow -> bag of words ('averaging info')\n",
    "\n",
    "#Inefficient implementation (using matrix multiplication will be much fasterdf)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "print(x[0])\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(dim=1, keepdim=True)\n",
    "print(wei)\n",
    "\n",
    "#Our dot product becomes an efficient weighted sum average across the previous terms.\n",
    "xbow2 = wei @ x # 8, 8 @ (4, 8, 2) -> Torch expands to 4, 8, 8 @ 4, 8, 2\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3rd version. Identical to the first version, except now we use the softmax (exponentiate then normalize)\n",
    "# By setting -inf on the terms, we ensure that we only learn on the non-inf elements\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "tril\n",
    "wei = torch.zeros(T,T)\n",
    "wei\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "print(x[0])\n",
    "print(xbow3[0])\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False) #We don't share the raw x values but rather v 'public' info about x\n",
    "k = key(x) # B, T, 16 (key -> given an context+pos embedding, we get 'multi-head'). Each context+pos gets different multi-head\n",
    "q = query(x) # B, T, 16 (query -> given an context+pos embedding, we get 'multi-head')\n",
    "\n",
    "# 'communication' between \n",
    "wei = q @ k.transpose(-2, -1) #(B, T, 16) @ (B, 16, T) -> (B, T, T) | Each we multiply every token multi-heads against each other\n",
    "\n",
    "# Here we create a mask to ensure each token only focuses on the previous context (and not polluted by future context)\n",
    "\n",
    "\n",
    "# DECODER architecture (in some cases, we want fully connected ie 'encoder' architectures)\n",
    "# Simple average of current token + all the previous tokens\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# wei = torch.zeros(T,T)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #only present in decoder block, without this it becomes encoder.\n",
    "wei = F.softmax(wei, dim=-1) #weirdly, I need dim=2 instead of dim=1 like karpathy\n",
    "\n",
    "v = value(x) #V is the thing that gets aggregated (and not the raw value of x)\n",
    "out = wei @ v\n",
    "\n",
    "# This should basically tell us how much each previous token context is important for the current token\n",
    "\n",
    "# Notes:\n",
    "# Different tokens find other tokens more/less interesting. We want to gather this in a data dependent way\n",
    "# I.e. we don't want everyone to be uniform/0s, we want to gather this in a data dependent way.\n",
    "\n",
    "# Every node sends a query and a key. Query -> what am i Looking for? Key -> What do I contain?\n",
    "# Dot product of key and queries (i.e. my query dot products with all other keys). If aligned, will interact higher thus I will learn about other tokens more than the others\n",
    "\n",
    "#Before wei was just a constant. But now every batch element will have different weis\n",
    "out.shape\n",
    "\n",
    "# Keys/Values could come from other blocks/nodes -> 'cross-attention'\n",
    "\n",
    "# Wei needs to be fairly diffuse (otherwise softmax converges 'one-hot' values thus we're only aggregating info from a single node)\n",
    "# Thus we need to scale with the sqrt(heads)\n",
    "# Before the softmax, make sure wei = wei * (head_size ** -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
